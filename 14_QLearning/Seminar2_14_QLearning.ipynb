{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxEdAOYX-Ext"
      },
      "source": [
        "# Семинар 14"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.path.exists(\".setup_complete\"):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ[\"DISPLAY\"] = \":1\""
      ],
      "metadata": {
        "id": "-uVrCHTToqeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXJy0PNb-Exw"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque, defaultdict\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from IPython.display import clear_output, display\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gym.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MzhZ0w1kHEOK",
        "outputId": "b618c358-a0cf-4072-d40a-268e40d4eb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.25.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabular Q-learning"
      ],
      "metadata": {
        "id": "Oagb1LHzq_-F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx5RcuOyrB-m"
      },
      "source": [
        "Реализуем алгоритм Q-learning для среды CliffWalking. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs import toy_text"
      ],
      "metadata": {
        "id": "LqgPC4qetbJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPhZNmd4rB-m"
      },
      "source": [
        "env = toy_text.CliffWalkingEnv()\n",
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=16nbZM2ZoY7xabN4uDEvCCz9IWmk_9D_x)"
      ],
      "metadata": {
        "id": "S1ZEyPCVt86g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JHUoKyfrB-m"
      },
      "source": [
        "print(env.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWAniYcrB-m"
      },
      "source": [
        "Так как награда выражатеся отрицательными значениями, то фактической целью агента является как можно более быстрое преодоление пути от старта к финишу при этом ему нужно не упасть с обрыва."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AalEO1PfrB-m"
      },
      "source": [
        "Традиционно посмотрим сколько очков в среднем за 100 эпизодов сможет набрать \"cлучайный\" агент."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(0)"
      ],
      "metadata": {
        "id": "hk8GxE7pDpE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQjmQwAsrB-n"
      },
      "source": [
        "total_reward = []\n",
        "total_length = []\n",
        "for episode in range(10):\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "    observation = env.reset()\n",
        "    for t in range(100):\n",
        "        # Ваш код здесь\n",
        "    total_reward.append(episode_reward)\n",
        "    total_length.append(episode_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsTj3pj4rB-n"
      },
      "source": [
        "print(np.mean(total_reward), np.mean(total_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seJZ2PVkrB-n"
      },
      "source": [
        "Ожидаемо, наш \"случайный\" агент просто блуждает по среде, не пытаясь добраться до цели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc5OmdQDrB-n"
      },
      "source": [
        "Вспомним как выглядит реализация данного алгоритма \n",
        "![](https://drive.google.com/uc?export=view&id=1eMG5HUaDyKKE8DQUhEdOaBZVR1UqOTO9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkHnQ5sjrB-n"
      },
      "source": [
        "Так как мы раелизуем табличную версию Q-learning, создадим структура, которая будет хранить значения нашей функции *Q(S,A)* для каждого состояния и действия. Она представляет собой словарь (*dict*), хранящий в качестве ключей состояния, а в качестве значений массив значений *Q-функции* для каждого действия для данного ключа-состояния."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SowXKlJXrB-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4888356d-b214-450b-9541-40a3c7adc453"
      },
      "source": [
        "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "print(\"Q-значения для состояния-действия (0, 0): %s\" % Q[(0, 0)], \"хранятся в списке, по значению для каждого действия\")\n",
        "print(\"Таким обарзом, Q-значение для действия 3 в в состоянии (1,2), i.e. Q((1,2), 3), можно получить вот так q_vals[(1,2)][3]:\", Q[(1,2)][3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-значения для состояния-действия (0, 0): [0. 0. 0. 0.] хранятся в списке, по значению для каждого действия\n",
            "Таким обарзом, Q-значение для действия 3 в в состоянии (1,2), i.e. Q((1,2), 3), можно получить вот так q_vals[(1,2)][3]: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzC5jvIrB-n"
      },
      "source": [
        "Сначала напишем функцию реализующую $\\epsilon$-greedy политику для исследования среды."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbSZzeJgrB-o"
      },
      "source": [
        "def eps_greedy(Q, state, epsilon):\n",
        "    \"\"\"\n",
        "    Параметры:\n",
        "        Q: таблица значений Q-функции\n",
        "        epsilon: параметр эпсилон\n",
        "        state: текущее состоние\n",
        "    Результат:\n",
        "        Случайное действие с вероятностью eps или argmax Q(s, .) c вероятностью (1 - eps)\n",
        "    \"\"\"\n",
        "    # Ваш код здесь\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNvNjx8urB-o"
      },
      "source": [
        "Также напишем функцию, которая поможет нам оценить, как ведет себя обученный агент."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6J3bywzrB-o"
      },
      "source": [
        "def q_eval(Q, render=False):\n",
        "    total_reward = []\n",
        "    for i_episode in range(10):\n",
        "        episode_reward = 0\n",
        "        observation = env.reset()\n",
        "        for t in range(100):\n",
        "            # Ваш код здесь\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                # print(\"Episode {} finished after {} timesteps\".format(i_episode+1, t+1))\n",
        "                break\n",
        "        total_reward.append(episode_reward)\n",
        "        # print(total_reward)\n",
        "    return np.mean(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_eval(Q)"
      ],
      "metadata": {
        "id": "ITUMKKae-C9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et1UkprirB-o"
      },
      "source": [
        "Теперь реализуем функцию обучения агента (обновления значений Q-функции) с использование функции, реализующей $\\epsilon$-greedy политику."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pmiwWqnrB-o"
      },
      "source": [
        "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"    \n",
        "    Параменты:\n",
        "        env: среда Open AI\n",
        "        num_episodes: Количество эпизодов\n",
        "        discount_factor: фактор дисконтирования\n",
        "        alpha: константа обучения\n",
        "        epsilon: параметр эпсилон для ϵ-greedy политику\n",
        "    \n",
        "    Результат:\n",
        "        Таблица с оптимальными значениями Q-функции\n",
        "    \"\"\"\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    episode_lengths=np.zeros(num_episodes)\n",
        "    episode_rewards=np.zeros(num_episodes)\n",
        "        \n",
        "    for i_episode in range(num_episodes):\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(\"\\rEpisode {}/{} score = {}.\".format(i_episode + 1, num_episodes, q_eval(Q)))\n",
        "        \n",
        "        state = env.reset()\n",
        "        \n",
        "        # Ваш код здесь\n",
        "    \n",
        "    return Q, episode_lengths, episode_rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wFpiP-_rB-o"
      },
      "source": [
        "Q, episode_lengths, episode_rewards = q_learning(env, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(episode_rewards)"
      ],
      "metadata": {
        "id": "Z9QvpDh3K1H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gtlY0jLrB-p"
      },
      "source": [
        "score = q_eval(Q)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDbZrAcxrB-p"
      },
      "source": [
        "plt.plot(episode_lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLzy87b9rB-p"
      },
      "source": [
        "plt.plot(episode_rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "rgFSL4zGrA29"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZyWoXy-Exy"
      },
      "source": [
        "## Q-learning algorithm\n",
        "\n",
        "Наша цель будет состоять в обучении политике, которая пытается максимизировать дисконтированные,\n",
        "накопительное вознаграждение\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$\n",
        "\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "Обновляем Q функцию исходя из того, что политика должна удовлетворять уравнению Беллмана\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "Разницу между правой и левой частями уравнения называем temporal difference error, $\\delta$:\n",
        "\n",
        "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
        "\n",
        "Именно эту ошибку мы и будем мимнимизировать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCRPSMFr-Exz"
      },
      "source": [
        "## Будем работать непосредственно с экраном игры, для этого объявим вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0').unwrapped"
      ],
      "metadata": {
        "id": "L-LpSTsLpYD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2gl1Y_3-Exz"
      },
      "source": [
        "Преобразование картинки в тензор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-08a1Fl-Ex0"
      },
      "outputs": [],
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl3A5g2U-Ex0"
      },
      "source": [
        "Считывание положения каретки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnLAqPSc-Ex1"
      },
      "outputs": [],
      "source": [
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UleAin-H-Ex1"
      },
      "source": [
        "Считывание экрана игры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbWB2SPa-Ex1"
      },
      "outputs": [],
      "source": [
        "def get_screen():\n",
        "    # gym возвращает экран с размером 400x600x3\n",
        "    # Transpose it into torch order (CHW).\n",
        "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "    \n",
        "    # Обрежем верх и низ картинки\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    \n",
        "    # Центрируем картинку по каретке\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "        \n",
        "    # Обрезаем лишнюю часть, чтобы получить квадратную область\n",
        "    screen = screen[:, :, slice_range]\n",
        "    \n",
        "    # Переводим картинку в тензор\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    \n",
        "    # Сжимаем картинку и добавляем размерность батча\n",
        "    return resize(screen).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVBjrRrP-Ex2"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ7VnrRw-Exx"
      },
      "source": [
        "## Replay Memory\n",
        "\n",
        "Мы будем использовать воспроизведения опыта для тренировки нашей DQN. Replay Memory хранит\n",
        "переходы, которые наблюдает агент, что позволяет нам повторно использовать эти данные\n",
        "потом. Для уменьшения корреляций между состояниями и переходами мы будем выбирать случайные переходы из предыдущих игр."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhaE0p4h-Exx"
      },
      "outputs": [],
      "source": [
        "# Создаем кортежи, в которых будем хранить наш опыт\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# Создаем буфер, в котором будем хранить наш опыт\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "       self.memory = deque([],maxlen=capacity)        \n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        self.memory.append(Transition(*args))        \n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size) \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Ple45u-Exy"
      },
      "source": [
        "## Создадим модель для апроксимации политики"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncHawd7v-Exy"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "      super(DQN, self).__init__()\n",
        "      # Ваш код здесь\n",
        "\n",
        "      def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "          return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "\n",
        "      convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "      convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "      linear_input_size = convw * convh * 32\n",
        "      self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = x.to(device)\n",
        "      # Ваш код здесь\n",
        "      \n",
        "      return self.head(x.view(x.size(0), -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZHRSJRd-Ex2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rH_eVbT5-Ex2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 400\n",
        "TARGET_UPDATE = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1500):\n",
        "  print(math.exp(-1. * i / EPS_DECAY))"
      ],
      "metadata": {
        "id": "y1Mr0-fq_BnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvI4ESAk-Ex2"
      },
      "outputs": [],
      "source": [
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "n_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lirg7Y4n-Ex3"
      },
      "source": [
        "Объявляем две модели, одна отвечает за политику во время игры, другая для предсказания награды"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zx2Zwmr-Ex3"
      },
      "outputs": [],
      "source": [
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(100000)\n",
        "\n",
        "episode_durations = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_done = 0"
      ],
      "metadata": {
        "id": "bIJs8qvtmjkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpWYGZGU-Ex3"
      },
      "source": [
        "Выбор действия"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBXjZccz-Ex3"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    print(eps_threshold)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwfY2a3Y-Ex3"
      },
      "source": [
        "Визуализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnNDg9ne-Ex4"
      },
      "outputs": [],
      "source": [
        "def plot_durations():\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 30:\n",
        "        means = durations_t.unfold(0, 30, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(29), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.0001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik_fUVH1-Ex4"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCUR5LOl-Ex4"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    \n",
        "    # batch-array of Transitions to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Парсим transitions чтобы сформировать обучающую выборку\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Вычисляем Q(s_t, a) - Модель предсказывает Q(s_t), \n",
        "    # затем мы выбираем Q, соответствующие сделанному действию\n",
        "    state_action_values = # Ваш код здесь    \n",
        "\n",
        "    # Вычисляем V(s_{t+1}) для всех следующих состояний target_net(non_final_next_states)\n",
        "    # берем максимальной значение\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = # Ваш код здесь\n",
        "    \n",
        "    # Вычисляем ожидаемые значения Q функции\n",
        "    expected_state_action_values = # Ваш код здесь\n",
        "\n",
        "    # Вычисляем Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Шаг оптимизации\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    #for param in policy_net.parameters():\n",
        "    #    param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9shX622-Ex4"
      },
      "source": [
        "## Основной цикл обучения\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFpl6MsJ-Ex4"
      },
      "outputs": [],
      "source": [
        "num_episodes = 500\n",
        "for i_episode in range(num_episodes):\n",
        "    \n",
        "    # Инициализация\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen    \n",
        "    \n",
        "    for t in count():\n",
        "        \n",
        "        # Выбираем действие\n",
        "\n",
        "        # Ваш код здесь\n",
        "\n",
        "        # Наблюдаем новое состояние\n",
        "\n",
        "        # Ваш код здесь\n",
        "\n",
        "        # Сохраняем transition в буфер\n",
        "\n",
        "        # Ваш код здесь\n",
        "\n",
        "        # Делаем шаг оптимизации\n",
        "\n",
        "        # Ваш код здесь\n",
        "            \n",
        "    # Обновляем target network, копирую веса из policy_net\n",
        "    \n",
        "    # Ваш код здесь\n",
        "\n",
        "print('Complete')\n",
        "# env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output, display\n",
        "import time\n",
        "\n",
        "env.reset()\n",
        "last_screen = get_screen()\n",
        "current_screen = get_screen()\n",
        "state = current_screen - last_screen    \n",
        "\n",
        "time_limit = 250\n",
        "for time_step in range(time_limit):\n",
        "    # Choose action based on your policy.\n",
        "    action = policy_net(state).max(1)[1].view(1, 1)\n",
        "    _, reward, done, _, _ = env.step(action.item())\n",
        "    print(action.item())\n",
        "\n",
        "    # Наблюдаем новое состояние\n",
        "    last_screen = current_screen\n",
        "    current_screen = get_screen()\n",
        "    if not done:\n",
        "        next_state = current_screen - last_screen\n",
        "    else:\n",
        "        next_state = None\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "    # Draw game image on display.\n",
        "    clear_output(wait=True)\n",
        "    plt.title(time_step)\n",
        "    plt.imshow(env.render(\"rgb_array\"))\n",
        "    plt.show()\n",
        "    time.sleep(.05)\n",
        "\n",
        "    if time_step > 200:\n",
        "        print(\"Well done!\")\n",
        "        break\n",
        "\n",
        "    if done:\n",
        "        print(\"you lose\")\n",
        "        break"
      ],
      "metadata": {
        "id": "od9QOWO26k4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}